"""
ê·¸ë˜ë””ì–¸íŠ¸ ì˜¤ë¥˜ ìˆ˜ì •ëœ Web3 AI Agent Fine-tuning ì½”ë“œ
"""

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import re
import os
from pathlib import Path

@dataclass
class Web3TrainingConfig:
    """Web3 AI Agent í›ˆë ¨ ì„¤ì •"""
    
    # ëª¨ë¸ ì„¤ì •
    base_model: str = "Qwen/Qwen2.5-1.5B-Instruct"
    max_length: int = 1024
    
    # í›ˆë ¨ ì„¤ì •
    num_epochs: int = 2
    batch_size: int = 1
    learning_rate: float = 5e-5  # ë” ì‘ì€ í•™ìŠµë¥ 
    warmup_steps: int = 50
    
    # LoRA ì„¤ì •
    use_lora: bool = True
    lora_r: int = 8
    lora_alpha: int = 16
    lora_dropout: float = 0.05

class Web3DataProcessor:
    """Web3 ëŒ€í™” ë°ì´í„° ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        
        # Web3 íŠ¹í™” í† í°ë“¤ ì¶”ê°€
        self.web3_tokens = [
            "<function_call>", "</function_call>",
            "<observation>", "</observation>",
        ]
        
        # í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€
        num_added = self.tokenizer.add_tokens(self.web3_tokens)
        print(f"Added {num_added} special tokens")
    
    def process_conversation(self, conversation_data: Dict) -> Dict[str, Any]:
        """ëŒ€í™” ë°ì´í„°ë¥¼ ëª¨ë¸ í›ˆë ¨ìš©ìœ¼ë¡œ ë³€í™˜"""
        
        conversations = conversation_data["conversations"]
        tools = json.loads(conversation_data["tools"]) if isinstance(conversation_data["tools"], str) else conversation_data["tools"]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = self._build_system_prompt(tools)
        
        # ëŒ€í™” ì´ë ¥ êµ¬ì„± - ê°„ë‹¨í•œ í˜•ì‹ ì‚¬ìš©
        conversation_text = f"System: {system_prompt}\n\n"
        
        for conv in conversations:
            role = conv["role"]
            content = conv["content"]
            
            if role == "user":
                conversation_text += f"User: {content}\n"
            elif role == "function_call":
                formatted_call = self._format_function_call(content)
                conversation_text += f"Assistant: {formatted_call}\n"
            elif role == "observation":
                conversation_text += f"System: <observation>{content}</observation>\n"
            elif role == "assistant":
                conversation_text += f"Assistant: {content}\n"
        
        # í† í°í™”
        tokenized = self.tokenizer(
            conversation_text,
            truncation=True,
            max_length=1024,
            padding=False,
            return_tensors="pt"
        )
        
        # ì…ë ¥ê³¼ ë¼ë²¨ì´ ê°™ì€ì§€ í™•ì¸
        input_ids = tokenized["input_ids"][0]
        attention_mask = tokenized["attention_mask"][0]
        
        return {
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": input_ids.clone()  # ë¼ë²¨ì€ ì…ë ¥ê³¼ ë™ì¼
        }
    
    def _build_system_prompt(self, tools: List[Dict]) -> str:
        """ë„êµ¬ ì •ë³´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        tools_desc = []
        for tool in tools:
            tool_info = f"{tool['name']}: {tool['description']}"
            tools_desc.append(tool_info)
        
        system_prompt = f"""You are a Web3 AI Agent. Available tools: {', '.join(tools_desc)}. Use <function_call>{{...}}</function_call> format for function calls."""
        
        return system_prompt
    
    def _format_function_call(self, function_call: str) -> str:
        """Function callì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            call_data = json.loads(function_call)
            formatted_call = f"<function_call>{json.dumps(call_data)}</function_call>"
            return formatted_call
        except:
            return f"<function_call>{function_call}</function_call>"

class Web3AgentTrainer:
    """Web3 AI Agent í›ˆë ¨ê¸°"""
    
    def __init__(self, config: Web3TrainingConfig):
        self.config = config
        
        print("ğŸ”§ Initializing tokenizer...")
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            config.base_model,
            trust_remote_code=True,
            use_fast=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì •
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.data_processor = Web3DataProcessor(self.tokenizer)
        
        print("ğŸ”§ Loading model...")
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            config.base_model,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # í† í° ì¶”ê°€ë¡œ ì¸í•œ ì„ë² ë”© í¬ê¸° ì¡°ì •
        original_size = self.model.get_input_embeddings().weight.size(0)
        self.model.resize_token_embeddings(len(self.tokenizer))
        new_size = self.model.get_input_embeddings().weight.size(0)
        print(f"Resized embeddings: {original_size} -> {new_size}")
        
        # LoRA ì„¤ì •
        if config.use_lora:
            self._setup_lora()
        else:
            # LoRA ì—†ì´ í›ˆë ¨í•˜ëŠ” ê²½ìš°, ì¼ë¶€ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
            self._setup_partial_training()
    
    def _setup_lora(self):
        """LoRA ì„¤ì •"""
        try:
            from peft import LoraConfig, get_peft_model, TaskType
            
            # ëª¨ë¸ì˜ target modules í™•ì¸
            target_modules = []
            for name, module in self.model.named_modules():
                if isinstance(module, torch.nn.Linear):
                    if any(target in name for target in ["q_proj", "v_proj", "k_proj", "o_proj"]):
                        target_modules.append(name.split('.')[-1])
            
            # ê¸°ë³¸ target modules ì‚¬ìš© (Qwen ëª¨ë¸ìš©)
            if not target_modules:
                target_modules = ["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            
            print(f"Target modules for LoRA: {target_modules}")
            
            lora_config = LoraConfig(
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                bias="none",
                task_type=TaskType.CAUSAL_LM,
                target_modules=target_modules
            )
            
            self.model = get_peft_model(self.model, lora_config)
            self.model.print_trainable_parameters()
            print("âœ… LoRA configuration applied")
            
        except ImportError:
            print("âŒ PEFT library not found. Installing...")
            os.system("pip3 install peft")
            print("Please restart the script after installation.")
            exit(1)
        except Exception as e:
            print(f"âŒ LoRA setup failed: {e}")
            print("Continuing without LoRA...")
            self._setup_partial_training()
    
    def _setup_partial_training(self):
        """LoRA ì—†ì´ ë¶€ë¶„ í›ˆë ¨ ì„¤ì •"""
        print("Setting up partial training (last layers only)...")
        
        # ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ frozenìœ¼ë¡œ ì„¤ì •
        for param in self.model.parameters():
            param.requires_grad = False
        
        # ë§ˆì§€ë§‰ ëª‡ ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        num_layers = len(self.model.model.layers) if hasattr(self.model.model, 'layers') else 32
        layers_to_train = max(2, num_layers // 8)  # ì „ì²´ ë ˆì´ì–´ì˜ 1/8 ë˜ëŠ” ìµœì†Œ 2ê°œ
        
        print(f"Training last {layers_to_train} layers out of {num_layers}")
        
        # ë§ˆì§€ë§‰ ë ˆì´ì–´ë“¤ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        if hasattr(self.model.model, 'layers'):
            for i in range(num_layers - layers_to_train, num_layers):
                for param in self.model.model.layers[i].parameters():
                    param.requires_grad = True
        
        # ì„ë² ë”©ê³¼ í—¤ë“œë„ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        if hasattr(self.model, 'lm_head'):
            for param in self.model.lm_head.parameters():
                param.requires_grad = True
        
        # ìƒˆë¡œ ì¶”ê°€ëœ í† í° ì„ë² ë”© í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ ì„¤ì •
        embedding_params = self.model.get_input_embeddings().weight
        embedding_params.requires_grad = True
        
        # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total_params = sum(p.numel() for p in self.model.parameters())
        print(f"Trainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)")
    
    def _create_sample_data(self) -> List[Dict]:
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        print("ğŸ“ Creating sample data...")
        
        sample_data = [
            {
                "conversations": [
                    {"role": "user", "content": "What's the price of ETH?"},
                    {"role": "function_call", "content": '{"name": "get_current_prices", "arguments": {"tokens": ["ETH"]}}'},
                    {"role": "observation", "content": '{"ETH": "$2000"}'},
                    {"role": "assistant", "content": "The current price of Ethereum (ETH) is $2,000."}
                ],
                "tools": '[{"name": "get_current_prices", "description": "Get current token prices", "parameters": {"type": "object", "properties": {"tokens": {"type": "array"}}, "required": ["tokens"]}}]'
            },
            {
                "conversations": [
                    {"role": "user", "content": "Show me popular DeFi tokens"},
                    {"role": "function_call", "content": '{"name": "get_trending_tokens", "arguments": {"category": "DeFi", "limit": 5}}'},
                    {"role": "observation", "content": '{"tokens": ["UNI", "AAVE", "COMP", "MKR", "SNX"]}'},
                    {"role": "assistant", "content": "Here are the top 5 trending DeFi tokens: UNI, AAVE, COMP, MKR, and SNX."}
                ],
                "tools": '[{"name": "get_trending_tokens", "description": "Get trending tokens by category", "parameters": {"type": "object", "properties": {"category": {"type": "string"}, "limit": {"type": "integer"}}, "required": ["category", "limit"]}}]'
            }
        ]
        
        # ë°ì´í„° ë³µì œ
        extended_data = sample_data * 20  # 40ê°œ ìƒ˜í”Œ
        return extended_data
    
    def prepare_dataset(self, conversation_data: List[Dict]) -> Dataset:
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        
        processed_data = []
        for i, conv_data in enumerate(conversation_data):
            try:
                processed = self.data_processor.process_conversation(conv_data)
                
                # í…ì„œ í¬ê¸° í™•ì¸
                if len(processed["input_ids"]) > 0:
                    processed_data.append(processed)
                    
            except Exception as e:
                print(f"âŒ Error processing conversation {i}: {e}")
                continue
        
        print(f"âœ… Successfully processed {len(processed_data)} conversations")
        return Dataset.from_list(processed_data)
    
    def train(self, dataset_path: str = None):
        """ëª¨ë¸ í›ˆë ¨"""
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        conversation_data = self._create_sample_data()
        dataset = self.prepare_dataset(conversation_data)
        
        if len(dataset) == 0:
            print("âŒ No valid data found!")
            return
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "./web3-agent-model"
        os.makedirs(output_dir, exist_ok=True)
        
        # í›ˆë ¨ ì¸ì ì„¤ì • - ë” ë³´ìˆ˜ì ìœ¼ë¡œ
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            gradient_accumulation_steps=4,  # effective batch size ì¦ê°€
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=5,
            save_steps=50,
            save_total_limit=2,
            logging_dir="./logs",
            report_to=None,
            dataloader_pin_memory=False,
            fp16=torch.cuda.is_available(),
            gradient_checkpointing=True,
            remove_unused_columns=False,
            dataloader_num_workers=0,
            max_grad_norm=1.0,
            optim="adamw_torch",
            weight_decay=0.01,
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,
            pad_to_multiple_of=8,
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # í›ˆë ¨ ì „ íŒŒë¼ë¯¸í„° ì²´í¬
        print("\nğŸ” Checking trainable parameters...")
        trainable_params = []
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                trainable_params.append(name)
        
        if not trainable_params:
            print("âŒ No trainable parameters found!")
            return
        
        print(f"âœ… Found {len(trainable_params)} trainable parameter groups")
        
        # í›ˆë ¨ ì‹œì‘
        print("ğŸš€ Starting Web3 AI Agent training...")
        try:
            trainer.train()
            
            # ëª¨ë¸ ì €ì¥
            trainer.save_model()
            self.tokenizer.save_pretrained(output_dir)
            
            print("âœ… Training completed successfully!")
            print(f"ğŸ“ Model saved to: {output_dir}")
            
        except Exception as e:
            print(f"âŒ Training failed: {e}")
            import traceback
            traceback.print_exc()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ Initializing Web3 AI Agent Fine-tuning...")
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"ğŸ”§ GPU: {torch.cuda.get_device_name()}")
        print(f"ğŸ”§ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # ì„¤ì •
    config = Web3TrainingConfig()
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Web3AgentTrainer(config)
    
    # í›ˆë ¨ ì‹¤í–‰
    trainer.train()

if __name__ == "__main__":
    main()