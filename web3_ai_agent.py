"""
ì‹¤í–‰ ê°€ëŠ¥í•œ Web3 AI Agent Fine-tuning ì½”ë“œ
"""

import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import json
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
import re
import os
from pathlib import Path

@dataclass
class Web3TrainingConfig:
    """Web3 AI Agent í›ˆë ¨ ì„¤ì •"""
    
    # ëª¨ë¸ ì„¤ì •
    base_model: str = "Qwen/Qwen2.5-1.5B-Instruct"
    max_length: int = 2048  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„
    
    # í›ˆë ¨ ì„¤ì •
    num_epochs: int = 3
    batch_size: int = 2  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¤„ì„
    learning_rate: float = 2e-4
    warmup_steps: int = 100
    
    # LoRA ì„¤ì •
    use_lora: bool = True
    lora_r: int = 16
    lora_alpha: int = 32
    lora_dropout: float = 0.1

class Web3DataProcessor:
    """Web3 ëŒ€í™” ë°ì´í„° ì „ì²˜ë¦¬ê¸°"""
    
    def __init__(self, tokenizer):
        self.tokenizer = tokenizer
        
        # Web3 íŠ¹í™” í† í°ë“¤ ì¶”ê°€
        self.web3_tokens = [
            "<function_call>", "</function_call>",
            "<observation>", "</observation>",
        ]
        
        # í† í¬ë‚˜ì´ì €ì— íŠ¹ìˆ˜ í† í° ì¶”ê°€
        num_added = self.tokenizer.add_tokens(self.web3_tokens)
        print(f"Added {num_added} special tokens")
    
    def process_conversation(self, conversation_data: Dict) -> Dict[str, Any]:
        """ëŒ€í™” ë°ì´í„°ë¥¼ ëª¨ë¸ í›ˆë ¨ìš©ìœ¼ë¡œ ë³€í™˜"""
        
        conversations = conversation_data["conversations"]
        tools = json.loads(conversation_data["tools"]) if isinstance(conversation_data["tools"], str) else conversation_data["tools"]
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        system_prompt = self._build_system_prompt(tools)
        
        # ëŒ€í™” ì´ë ¥ êµ¬ì„±
        conversation_text = f"<|im_start|>system\n{system_prompt}<|im_end|>\n"
        
        for conv in conversations:
            role = conv["role"]
            content = conv["content"]
            
            if role == "user":
                conversation_text += f"<|im_start|>user\n{content}<|im_end|>\n"
            
            elif role == "function_call":
                # Function callì„ íŠ¹ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted_call = self._format_function_call(content)
                conversation_text += f"<|im_start|>assistant\n{formatted_call}<|im_end|>\n"
            
            elif role == "observation":
                # API ê²°ê³¼ë¥¼ íŠ¹ìˆ˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                formatted_obs = f"<observation>{content}</observation>"
                conversation_text += f"<|im_start|>system\n{formatted_obs}<|im_end|>\n"
            
            elif role == "assistant":
                conversation_text += f"<|im_start|>assistant\n{content}<|im_end|>\n"
        
        # í† í°í™”
        tokenized = self.tokenizer(
            conversation_text,
            truncation=True,
            max_length=2048,
            padding=False,  # ë™ì  íŒ¨ë”© ì‚¬ìš©
            return_tensors="pt"
        )
        
        return {
            "input_ids": tokenized["input_ids"][0],
            "attention_mask": tokenized["attention_mask"][0],
            "labels": tokenized["input_ids"][0].clone()  # ë¼ë²¨ì€ ì…ë ¥ê³¼ ë™ì¼
        }
    
    def _build_system_prompt(self, tools: List[Dict]) -> str:
        """ë„êµ¬ ì •ë³´ë¥¼ í¬í•¨í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        
        tools_desc = []
        for tool in tools:
            tool_info = f"{tool['name']}: {tool['description']}"
            tools_desc.append(tool_info)
        
        system_prompt = f"""You are a Web3 AI Agent specialized in blockchain and DeFi operations.

Available Tools:
{chr(10).join(tools_desc)}

When you need to call a function, use this format:
<function_call>
{{"name": "function_name", "arguments": {{"param": "value"}}}}
</function_call>"""
        
        return system_prompt
    
    def _format_function_call(self, function_call: str) -> str:
        """Function callì„ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            # JSON íŒŒì‹±í•˜ì—¬ ê²€ì¦
            call_data = json.loads(function_call)
            formatted_call = f"<function_call>\n{json.dumps(call_data, indent=2)}\n</function_call>"
            return formatted_call
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜
            return f"<function_call>\n{function_call}\n</function_call>"

class Web3AgentTrainer:
    """Web3 AI Agent í›ˆë ¨ê¸°"""
    
    def __init__(self, config: Web3TrainingConfig):
        self.config = config
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(config.base_model)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ë°ì´í„° í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
        self.data_processor = Web3DataProcessor(self.tokenizer)
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = AutoModelForCausalLM.from_pretrained(
            config.base_model,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )
        
        # í† í° ì¶”ê°€ë¡œ ì¸í•œ ì„ë² ë”© í¬ê¸° ì¡°ì •
        self.model.resize_token_embeddings(len(self.tokenizer))
        
        # LoRA ì„¤ì •
        if config.use_lora:
            self._setup_lora()
    
    def _setup_lora(self):
        """LoRA ì„¤ì •"""
        try:
            from peft import LoraConfig, get_peft_model, TaskType
            
            lora_config = LoraConfig(
                r=self.config.lora_r,
                lora_alpha=self.config.lora_alpha,
                lora_dropout=self.config.lora_dropout,
                bias="none",
                task_type=TaskType.CAUSAL_LM,
                target_modules=["q_proj", "v_proj", "k_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
            )
            
            self.model = get_peft_model(self.model, lora_config)
            print("âœ… LoRA configuration applied")
        except ImportError:
            print("âŒ PEFT library not found. Please install: pip install peft")
            print("Continuing without LoRA...")
    
    def load_dataset_from_file(self, file_path: str) -> List[Dict]:
        """íŒŒì¼ì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ"""
        if not os.path.exists(file_path):
            print(f"âŒ Dataset file not found: {file_path}")
            return self._create_sample_data()
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            print(f"âœ… Loaded {len(data)} conversations from {file_path}")
            return data
        except Exception as e:
            print(f"âŒ Error loading dataset: {e}")
            return self._create_sample_data()
    
    def _create_sample_data(self) -> List[Dict]:
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„°ê°€ ì—†ì„ ë•Œ)"""
        print("ğŸ“ Creating sample data for demonstration...")
        
        sample_data = [
            {
                "conversations": [
                    {"role": "user", "content": "I want to invest 5 ETH across the top 3 AI tokens with the highest social media momentum."},
                    {"role": "function_call", "content": '{"name": "get_social_media_momentum", "arguments": {"category": "AI", "limit": 3}}'},
                    {"role": "observation", "content": '{"tokens": ["AI Token1", "AI Token2", "AI Token3"]}'},
                    {"role": "function_call", "content": '{"name": "create_portfolio", "arguments": {"total_eth": 5, "tokens": ["AI Token1", "AI Token2", "AI Token3"]}}'},
                    {"role": "observation", "content": '{"allocation": {"AI Token1": "1.67 ETH", "AI Token2": "1.67 ETH", "AI Token3": "1.66 ETH"}}'},
                    {"role": "assistant", "content": "Successfully created a balanced portfolio allocation across the top 3 AI tokens based on social media momentum."}
                ],
                "tools": '[{"name": "get_social_media_momentum", "description": "Get social media momentum for tokens", "parameters": {"type": "object", "properties": {"category": {"type": "string"}, "limit": {"type": "integer"}}, "required": ["category", "limit"]}}, {"name": "create_portfolio", "description": "Create investment portfolio", "parameters": {"type": "object", "properties": {"total_eth": {"type": "number"}, "tokens": {"type": "array"}}, "required": ["total_eth", "tokens"]}}]'
            },
            {
                "conversations": [
                    {"role": "user", "content": "What's the current price of Bitcoin?"},
                    {"role": "function_call", "content": '{"name": "get_current_prices", "arguments": {"tokens": ["BTC"]}}'},
                    {"role": "observation", "content": '{"BTC": "$45,000"}'},
                    {"role": "assistant", "content": "The current price of Bitcoin (BTC) is $45,000."}
                ],
                "tools": '[{"name": "get_current_prices", "description": "Get current token prices", "parameters": {"type": "object", "properties": {"tokens": {"type": "array"}}, "required": ["tokens"]}}]'
            }
        ]
        
        # ë°ì´í„° ë³µì œí•˜ì—¬ ë” ë§ì€ í›ˆë ¨ ìƒ˜í”Œ ìƒì„±
        extended_data = []
        for _ in range(50):  # 100ê°œ ìƒ˜í”Œ ìƒì„±
            extended_data.extend(sample_data)
        
        return extended_data
    
    def prepare_dataset(self, conversation_data: List[Dict]) -> Dataset:
        """ë°ì´í„°ì…‹ ì¤€ë¹„"""
        
        processed_data = []
        for i, conv_data in enumerate(conversation_data):
            try:
                processed = self.data_processor.process_conversation(conv_data)
                processed_data.append(processed)
            except Exception as e:
                print(f"âŒ Error processing conversation {i}: {e}")
                continue
        
        print(f"âœ… Successfully processed {len(processed_data)} conversations")
        return Dataset.from_list(processed_data)
    
    def train(self, dataset_path: str = None):
        """ëª¨ë¸ í›ˆë ¨"""
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        if dataset_path:
            conversation_data = self.load_dataset_from_file(dataset_path)
        else:
            conversation_data = self._create_sample_data()
        
        # ë°ì´í„°ì…‹ ì¤€ë¹„
        dataset = self.prepare_dataset(conversation_data)
        
        # í›ˆë ¨/ê²€ì¦ ë¶„í• 
        train_size = int(0.9 * len(dataset))
        train_dataset = dataset.select(range(train_size)) if train_size > 0 else dataset
        eval_dataset = dataset.select(range(train_size, len(dataset))) if len(dataset) > train_size else None
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = "./web3-agent-model"
        os.makedirs(output_dir, exist_ok=True)
        
        # í›ˆë ¨ ì¸ì ì„¤ì •
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=self.config.num_epochs,
            per_device_train_batch_size=self.config.batch_size,
            per_device_eval_batch_size=self.config.batch_size,
            learning_rate=self.config.learning_rate,
            warmup_steps=self.config.warmup_steps,
            logging_steps=10,
            save_steps=100,
            save_total_limit=2,
            logging_dir="./logs",
            report_to=None,
            dataloader_pin_memory=False,
            fp16=torch.cuda.is_available(),
            gradient_checkpointing=True,
            remove_unused_columns=False,
        )
        
        # ë°ì´í„° ì½œë ˆì´í„°
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=self.tokenizer,
            mlm=False,  # Causal LMì´ë¯€ë¡œ False
        )
        
        # íŠ¸ë ˆì´ë„ˆ ìƒì„±
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
        )
        
        # í›ˆë ¨ ì‹œì‘
        print("ğŸš€ Starting Web3 AI Agent training...")
        try:
            trainer.train()
            
            # ëª¨ë¸ ì €ì¥
            trainer.save_model()
            self.tokenizer.save_pretrained(output_dir)
            
            print("âœ… Training completed successfully!")
            print(f"ğŸ“ Model saved to: {output_dir}")
            
        except Exception as e:
            print(f"âŒ Training failed: {e}")
            raise

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ”§ Initializing Web3 AI Agent Fine-tuning...")
    
    # ì„¤ì •
    config = Web3TrainingConfig()
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Web3AgentTrainer(config)
    
    # í›ˆë ¨ ì‹¤í–‰
    # dataset.json íŒŒì¼ì´ ìˆë‹¤ë©´ í•´ë‹¹ íŒŒì¼ì„ ì‚¬ìš©, ì—†ë‹¤ë©´ ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©
    trainer.train(dataset_path="dataset.json")

if __name__ == "__main__":
    main()