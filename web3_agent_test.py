"""
Web3 AI Agent ì¶”ë¡  ì „ìš© ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import sys

def load_trained_model(model_path="./web3-agent-model"):
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    
    if not os.path.exists(model_path):
        print(f"âŒ Model not found at {model_path}")
        print("Please run the training script first: python3 train.py")
        return None, None
    
    print(f"ğŸ“š Loading model from {model_path}...")
    
    try:
        # PEFT ì–´ëŒ‘í„° íŒŒì¼ í™•ì¸
        adapter_files = ["adapter_model.bin", "adapter_model.safetensors", "pytorch_model.bin"]
        is_peft_model = any(os.path.exists(os.path.join(model_path, f)) for f in adapter_files[:2])
        
        if is_peft_model:
            # PEFT ëª¨ë¸ì¸ ê²½ìš°
            try:
                from peft import PeftModel
                
                print("ğŸ”§ Loading PEFT model...")
                
                # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
                base_model = AutoModelForCausalLM.from_pretrained(
                    "Qwen/Qwen2.5-1.5B-Instruct",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None,
                    trust_remote_code=True
                )
                
                # íŠ¹ìˆ˜ í† í°ì„ ìœ„í•œ ì„ë² ë”© í¬ê¸° ì¡°ì •
                tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct", trust_remote_code=True)
                special_tokens = ["<function_call>", "</function_call>", "<observation>", "</observation>"]
                tokenizer.add_tokens(special_tokens)
                base_model.resize_token_embeddings(len(tokenizer))
                
                # PEFT ì–´ëŒ‘í„° ë¡œë“œ
                model = PeftModel.from_pretrained(base_model, model_path)
                print("âœ… PEFT model loaded")
                
                # ì €ì¥ëœ í† í¬ë‚˜ì´ì €ê°€ ìˆìœ¼ë©´ ì‚¬ìš©
                try:
                    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
                except:
                    pass
                
            except ImportError:
                print("âŒ PEFT library required for this model")
                print("Install with: pip install peft")
                return None, None
        else:
            # ì¼ë°˜ ëª¨ë¸ì¸ ê²½ìš°
            print("ğŸ”§ Loading standard model...")
            
            # config.json ìˆ˜ì • (í•„ìš”í•œ ê²½ìš°)
            config_path = os.path.join(model_path, "config.json")
            if os.path.exists(config_path):
                import json
                with open(config_path, 'r') as f:
                    config = json.load(f)
                
                if "model_type" not in config:
                    config["model_type"] = "qwen2"
                    with open(config_path, 'w') as f:
                        json.dump(config, f, indent=2)
                    print("âœ… Fixed config.json")
            
            # í† í¬ë‚˜ì´ì € ë¨¼ì € ë¡œë“œ
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True
            )
            
            # ëª¨ë¸ ë¡œë“œ
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            print("âœ… Standard model loaded")
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # GPU ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"ğŸ”§ Using GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ”§ Model device: {next(model.parameters()).device}")
        else:
            print("ğŸ”§ Using CPU")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        print("\nğŸ”§ Troubleshooting:")
        print("1. Make sure training completed successfully")
        print("2. Check if model files exist in the directory")
        print("3. Try running training again: python3 train.py")
        return None, None

def generate_response(model, tokenizer, user_input, max_new_tokens=200):
    """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
    
    # ì…ë ¥ ì „ì²˜ë¦¬
    prompt = f"User: {user_input}\nAssistant:"
    
    # í† í°í™”
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # GPUë¡œ ì´ë™ (ëª¨ë¸ì´ GPUì— ìˆëŠ” ê²½ìš°)
    if torch.cuda.is_available() and next(model.parameters()).is_cuda:
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # ìƒì„±
    model.eval()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # ë””ì½”ë”©
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if "Assistant:" in full_response:
        response = full_response.split("Assistant:")[-1].strip()
    else:
        response = full_response[len(prompt):].strip()
    
    return response

def interactive_chat(model, tokenizer):
    """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    
    print("\nğŸ¤– Web3 AI Agent is ready!")
    print("ğŸ’¡ Commands:")
    print("  - Type your question and press Enter")
    print("  - 'quit' or 'exit' to quit")
    print("  - 'clear' to clear screen")
    print("  - 'test' to run test cases")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ You: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            elif user_input.lower() == 'clear':
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif user_input.lower() == 'test':
                run_test_cases(model, tokenizer)
                continue
            elif not user_input:
                continue
            
            print("ğŸ¤” Thinking...", end="", flush=True)
            
            response = generate_response(model, tokenizer, user_input)
            print(f"\rğŸ¤– Agent: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")

def run_test_cases(model, tokenizer):
    """ë¯¸ë¦¬ ì •ì˜ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
    
    test_cases = [
        "What's the price of ETH?",
        "Show me popular DeFi tokens",
        "I want to swap 1 ETH for USDC",
        "Check my wallet balance",
        "What are the gas fees right now?",
        "Find the best yield farming opportunities",
        "Get information about Uniswap protocol"
    ]
    
    print("\nğŸ§ª Running test cases...")
    print("=" * 50)
    
    for i, test_input in enumerate(test_cases, 1):
        print(f"\nğŸ“ Test {i}: {test_input}")
        try:
            response = generate_response(model, tokenizer, test_input, max_new_tokens=150)
            print(f"ğŸ¤– Response: {response}")
        except Exception as e:
            print(f"âŒ Error: {e}")
        
        print("-" * 30)

def single_query_mode(model, tokenizer, query):
    """ë‹¨ì¼ ì¿¼ë¦¬ ëª¨ë“œ"""
    
    print(f"ğŸ“ Query: {query}")
    try:
        response = generate_response(model, tokenizer, query)
        print(f"ğŸ¤– Response: {response}")
    except Exception as e:
        print(f"âŒ Error: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸš€ Web3 AI Agent Inference")
    print("=" * 40)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_trained_model()
    
    if model is None or tokenizer is None:
        return
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            run_test_cases(model, tokenizer)
        else:
            # ë‹¨ì¼ ì¿¼ë¦¬ ì‹¤í–‰
            query = " ".join(sys.argv[1:])
            single_query_mode(model, tokenizer, query)
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        interactive_chat(model, tokenizer)

if __name__ == "__main__":
    main()