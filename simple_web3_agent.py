"""
ê°„ë‹¨í•˜ê³  í™•ì‹¤íˆ ì‘ë™í•˜ëŠ” Web3 AI Agent Fine-tuning ì½”ë“œ
"""

import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    Trainer, TrainingArguments
)
from datasets import Dataset
import json
import os

def load_training_data(file_path="training_set.jsonl"):
    """training_set.jsonl íŒŒì¼ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    if not os.path.exists(file_path):
        print(f"âŒ Training file not found: {file_path}")
        print("Creating sample data instead...")
        return create_fallback_data()
    
    data = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    json_data = json.loads(line.strip())
                    
                    # ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    text = convert_conversation_to_text(json_data)
                    if text:
                        data.append({"text": text})
                        
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSON parsing error on line {line_num}: {e}")
                except Exception as e:
                    print(f"âš ï¸ Error processing line {line_num}: {e}")
        
        print(f"âœ… Loaded {len(data)} conversations from {file_path}")
        return data
        
    except Exception as e:
        print(f"âŒ Error reading file {file_path}: {e}")
        return create_fallback_data()

def convert_conversation_to_text(json_data):
    """JSON ëŒ€í™” ë°ì´í„°ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    try:
        conversations = json_data.get("conversations", [])
        if not conversations:
            return None
        
        text_parts = []
        
        for conv in conversations:
            role = conv.get("role", "")
            content = conv.get("content", "")
            
            if role == "user":
                text_parts.append(f"User: {content}")
            elif role == "function_call":
                text_parts.append(f"Assistant: <function_call>{content}</function_call>")
            elif role == "observation":
                text_parts.append(f"System: <observation>{content}</observation>")
            elif role == "assistant":
                text_parts.append(f"Assistant: {content}")
        
        return "\n".join(text_parts)
        
    except Exception as e:
        print(f"âš ï¸ Error converting conversation: {e}")
        return None

def create_fallback_data():
    """íŒŒì¼ì´ ì—†ì„ ë•Œ ì‚¬ìš©í•  í´ë°± ë°ì´í„°"""
    return [
        {
            "text": "User: What's the price of ETH?\nAssistant: <function_call>{\"name\": \"get_current_prices\", \"arguments\": {\"tokens\": [\"ETH\"]}}</function_call>\nSystem: <observation>{\"ETH\": \"$2000\"}</observation>\nAssistant: The current price of Ethereum (ETH) is $2,000."
        },
        {
            "text": "User: Show me popular DeFi tokens\nAssistant: <function_call>{\"name\": \"get_trending_tokens\", \"arguments\": {\"category\": \"DeFi\", \"limit\": 5}}</function_call>\nSystem: <observation>{\"tokens\": [\"UNI\", \"AAVE\", \"COMP\", \"MKR\", \"SNX\"]}</observation>\nAssistant: Here are the top 5 trending DeFi tokens: UNI, AAVE, COMP, MKR, and SNX."
        }
    ] * 20  # 40ê°œ ìƒ˜í”Œ

def tokenize_function(examples, tokenizer):
    """í† í°í™” í•¨ìˆ˜"""
    # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”
    tokenized = tokenizer(
        examples["text"],
        truncation=True,
        padding=False,
        max_length=512,
        return_tensors=None  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    )
    
    # ë¼ë²¨ ì„¤ì • (ì…ë ¥ê³¼ ë™ì¼)
    tokenized["labels"] = tokenized["input_ids"].copy()
    
    return tokenized

def main():
    print("ğŸš€ Starting simple Web3 AI Agent fine-tuning...")
    
    # GPU í™•ì¸
    if torch.cuda.is_available():
        print(f"ğŸ”§ GPU: {torch.cuda.get_device_name()}")
        torch.cuda.empty_cache()
    
    # ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    
    print("ğŸ“š Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # Web3 íŠ¹í™” í† í° ì¶”ê°€
    special_tokens = ["<function_call>", "</function_call>", "<observation>", "</observation>"]
    tokenizer.add_tokens(special_tokens)
    
    print("ğŸ¤– Loading model...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # í† í° í¬ê¸° ì¡°ì •
    model.resize_token_embeddings(len(tokenizer))
    
    # PEFT ì„¤ì • (ê°„ë‹¨í•œ ë°©ì‹)
    try:
        from peft import LoraConfig, get_peft_model, TaskType
        
        peft_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=8,
            lora_alpha=16,
            lora_dropout=0.1,
            target_modules=["q_proj", "v_proj", "k_proj", "o_proj"]
        )
        
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        
    except ImportError:
        print("âš ï¸ PEFT not available, using full fine-tuning")
        # ì¼ë¶€ ë ˆì´ì–´ë§Œ í•™ìŠµ
        for param in model.parameters():
            param.requires_grad = False
        
        # ë§ˆì§€ë§‰ 2ê°œ ë ˆì´ì–´ë§Œ í•™ìŠµ
        for param in model.model.layers[-2:]:
            for p in param.parameters():
                p.requires_grad = True
        
        # í—¤ë“œ í•™ìŠµ
        for param in model.lm_head.parameters():
            param.requires_grad = True
    
    # ë°ì´í„° ì¤€ë¹„
    print("ğŸ“ Loading training dataset...")
    raw_data = load_training_data("training_set.jsonl")
    
    if not raw_data:
        print("âŒ No valid training data found!")
        return
    
    dataset = Dataset.from_list(raw_data)
    
    # í† í°í™”
    tokenized_dataset = dataset.map(
        lambda x: tokenize_function(x, tokenizer),
        batched=True,
        remove_columns=dataset.column_names
    )
    
    print(f"âœ… Dataset prepared: {len(tokenized_dataset)} samples")
    
    # í›ˆë ¨ ì„¤ì •
    training_args = TrainingArguments(
        output_dir="./web3-agent-simple",
        num_train_epochs=2,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=8,
        learning_rate=5e-5,
        warmup_steps=10,
        logging_steps=2,
        save_steps=20,
        save_total_limit=2,
        fp16=True,
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        gradient_checkpointing=False,  # ë¹„í™œì„±í™”
        report_to=None,
    )
    
    # ì»¤ìŠ¤í…€ ë°ì´í„° ì½œë ˆì´í„°
    def data_collator(features):
        # ìµœëŒ€ ê¸¸ì´ ê³„ì‚°
        max_length = max(len(f["input_ids"]) for f in features)
        
        batch = {
            "input_ids": [],
            "attention_mask": [],
            "labels": []
        }
        
        for feature in features:
            input_ids = feature["input_ids"]
            attention_mask = feature["attention_mask"]
            labels = feature["labels"]
            
            # íŒ¨ë”©
            padding_length = max_length - len(input_ids)
            
            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
            attention_mask = attention_mask + [0] * padding_length
            labels = labels + [-100] * padding_length
            
            batch["input_ids"].append(input_ids)
            batch["attention_mask"].append(attention_mask)
            batch["labels"].append(labels)
        
        # í…ì„œë¡œ ë³€í™˜
        return {
            "input_ids": torch.tensor(batch["input_ids"], dtype=torch.long),
            "attention_mask": torch.tensor(batch["attention_mask"], dtype=torch.long),
            "labels": torch.tensor(batch["labels"], dtype=torch.long)
        }
    
    # íŠ¸ë ˆì´ë„ˆ ìƒì„±
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )
    
    # í›ˆë ¨ ì‹œì‘
    print("ğŸš€ Starting training...")
    try:
        trainer.train()
        
        # ëª¨ë¸ ì €ì¥
        trainer.save_model()
        tokenizer.save_pretrained("./web3-agent-simple")
        
        print("âœ… Training completed successfully!")
        print("ğŸ“ Model saved to: ./web3-agent-simple")
        
        # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª Testing the model...")
        model.eval()
        
        test_input = "User: What's the price of Bitcoin?"
        inputs = tokenizer(test_input, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"Input: {test_input}")
        print(f"Output: {response}")
        
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()