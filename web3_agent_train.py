"""
Web3 AI Agent ì¶”ë¡  ì „ìš© ìŠ¤í¬ë¦½íŠ¸
"""

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os
import sys

def load_trained_model(model_path="./web3-agent-model"):
    """í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ"""
    
    if not os.path.exists(model_path):
        print(f"âŒ Model not found at {model_path}")
        print("Please run the training script first: python3 train.py")
        return None, None
    
    print(f"ğŸ“š Loading model from {model_path}...")
    
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        # ëª¨ë¸ ë¡œë“œ
        if os.path.exists(f"{model_path}/adapter_model.bin") or os.path.exists(f"{model_path}/adapter_model.safetensors"):
            # PEFT ëª¨ë¸ì¸ ê²½ìš°
            try:
                from peft import PeftModel
                
                # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
                base_model = AutoModelForCausalLM.from_pretrained(
                    "Qwen/Qwen2.5-1.5B-Instruct",
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto" if torch.cuda.is_available() else None,
                    trust_remote_code=True
                )
                
                # PEFT ì–´ëŒ‘í„° ë¡œë“œ
                model = PeftModel.from_pretrained(base_model, model_path)
                print("âœ… PEFT model loaded")
                
            except ImportError:
                print("âŒ PEFT library required for this model")
                return None, None
        else:
            # ì¼ë°˜ ëª¨ë¸ì¸ ê²½ìš°
            model = AutoModelForCausalLM.from_pretrained(
                model_path,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                trust_remote_code=True
            )
            print("âœ… Standard model loaded")
        
        # GPU ì •ë³´ ì¶œë ¥
        if torch.cuda.is_available():
            print(f"ğŸ”§ Using GPU: {torch.cuda.get_device_name()}")
            print(f"ğŸ”§ Model device: {next(model.parameters()).device}")
        else:
            print("ğŸ”§ Using CPU")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ Error loading model: {e}")
        return None, None

def generate_response(model, tokenizer, user_input, max_new_tokens=200):
    """ì‚¬ìš©ì ì…ë ¥ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±"""
    
    # ì…ë ¥ ì „ì²˜ë¦¬
    prompt = f"User: {user_input}\nAssistant:"
    
    # í† í°í™”
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # GPUë¡œ ì´ë™ (ëª¨ë¸ì´ GPUì— ìˆëŠ” ê²½ìš°)
    if torch.cuda.is_available() and next(model.parameters()).is_cuda:
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # ìƒì„±
    model.eval()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_p=0.9,
            repetition_penalty=1.1,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    
    # ë””ì½”ë”©
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
    if "Assistant:" in full_response:
        response = full_response.split("Assistant:")[-1].strip()
    else:
        response = full_response[len(prompt):].strip()
    
    return response

def interactive_chat(model, tokenizer):
    """ëŒ€í™”í˜• ì±„íŒ… ì¸í„°í˜ì´ìŠ¤"""
    
    print("\nğŸ¤– Web3 AI Agent is ready!")
    print("ğŸ’¡ Commands:")
    print("  - Type your question and press Enter")
    print("  - 'quit' or 'exit' to quit")
    print("  - 'clear' to clear screen")
    print("  - 'test' to run test cases")
    print("-" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ You: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ Goodbye!")
                break
            elif user_input.lower() == 'clear':
                os.system('clear' if os.name == 'posix' else 'cls')
                continue
            elif user_input.lower() == 'test':
                run_test_cases(model, tokenizer)
                continue
            elif not user_input:
                continue
            
            print("ğŸ¤” Thinking...", end="", flush=True)
            
            response = generate_response(model, tokenizer, user_input)
            print(f"\rğŸ¤– Agent: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ Error: {e}")

def run_test_cases(model, tokenizer):
    """ë¯¸ë¦¬ ì •ì˜ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰"""
    
    test_cases = [
        "What's the price of ETH?",
        "Show me popular DeFi tokens",
        "I want to swap 1 ETH for USDC",
        "Check my wallet balance",
        "What are the gas fees right now?",
        "Find the best yield farming opportunities",
        "Get information about Uniswap protocol"
    ]
    
    print("\nğŸ§ª Running test cases...")
    print("=" * 50)
    
    for i, test_input in enumerate(test_cases, 1):
        print(f"\nğŸ“ Test {i}: {test_input}")
        try:
            response = generate_response(model, tokenizer, test_input, max_new_tokens=150)
            print(f"ğŸ¤– Response: {response}")
        except Exception as e:
            print(f"âŒ Error: {e}")
        
        print("-" * 30)

def single_query_mode(model, tokenizer, query):
    """ë‹¨ì¼ ì¿¼ë¦¬ ëª¨ë“œ"""
    
    print(f"ğŸ“ Query: {query}")
    try:
        response = generate_response(model, tokenizer, query)
        print(f"ğŸ¤– Response: {response}")
    except Exception as e:
        print(f"âŒ Error: {e}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    print("ğŸš€ Web3 AI Agent Inference")
    print("=" * 40)
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_trained_model()
    
    if model is None or tokenizer is None:
        return
    
    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        if sys.argv[1] == "test":
            run_test_cases(model, tokenizer)
        else:
            # ë‹¨ì¼ ì¿¼ë¦¬ ì‹¤í–‰
            query = " ".join(sys.argv[1:])
            single_query_mode(model, tokenizer, query)
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        interactive_chat(model, tokenizer)

if __name__ == "__main__":
    main()